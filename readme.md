- This is only for learning the GOT-ocr principle using onnxLLM.
- No acceleration effect, master branch has onnx inference with kvcache
- first use llm-export\llm_export.py to export onnx, then run OnnxLLM\examples\got.py
- v1 branch is the first version that uses inference without kvcache
- c++ version: use mnn-llm for inference https://github.com/BaofengZan/mnn-llm-GOT-OCR2.0

References:

[Ucas-HaoranWei/GOT-OCR2.0: Official code implementation of General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model (github.com)](https://github.com/Ucas-HaoranWei/GOT-OCR2.0)

[wangzhaode/llm-export: llm-export can export llm model to onnx. (github.com)](https://github.com/wangzhaode/llm-export)

[inisis/OnnxLLM: Large Language Model Onnx Inference Framework (github.com)](https://github.com/inisis/OnnxLLM)

